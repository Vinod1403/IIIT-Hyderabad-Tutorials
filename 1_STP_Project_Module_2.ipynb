{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vinod1403/IIIT-Hyderabad-Tutorials/blob/main/1_STP_Project_Module_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LmWIufUKnVK6"
      },
      "source": [
        "# Module 2: Appreciating, Interpreting and Visualizing Data\n",
        "\n",
        "**Project: Understanding Customer Segments for Targeted Marketing**\n",
        "\n",
        "Introduction: The Power of Customer Segmentation\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Welcome to your Module 2 project!\n",
        "\n",
        "In today's competitive landscape, understanding your customers is paramount for any business. Generic marketing strategies often fall flat, but by truly appreciating the diverse needs and behaviors within your customer base, businesses can create more effective, personalized experiences. This process is known as **customer segmentation**,\n",
        "\n",
        "Customer segmentation involves dividing a broad customer base into subgroups of consumers who have common needs, interests, and priorities. By segmenting customers, companies can:\n",
        "\n",
        "* **Tailor Marketing Messages:** Design specific campaigns that resonate with each group.\n",
        "* **Optimize Product Development:** Create products and services that meet the unique demands of different segments.\n",
        "* **Improve Customer Service:** Provide support that addresses common issues for particular groups.\n",
        "* **Identify High-Value Customers:** Focus resources on segments that drive the most revenue.\n",
        "* **Predict Churn:** Identify customers at risk of leaving and intervene proactively."
      ],
      "metadata": {
        "id": "rsotc2kElmbE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, your task is to analyze a dataset of customer activity, use dimensionality reduction techniques to visualize customer behavior, and ultimately identify distinct customer segments. This will demonstrate how data visualization can provide actionable insights for business strategy, even without deep domain expertise at the outset.\n",
        "\n",
        "We will first focus on a synthetic dataset containing various metrics related to customer purchasing habits and engagement. Your goal will be to:\n",
        "* Process and prepare the raw customer data.\n",
        "* Use **Principal Component Analysis (PCA)** to understand the main drivers of customer variation.\n",
        "* Employ **t-Distributed Stochastic Neighbor Embedding (t-SNE)** to uncover hidden clusters of similar customers.\n",
        "* (Optional Challenge) Explore **Uniform Manifold Approximation and Projection (UMAP)** for an alternative perspective.\n",
        "* Interpret these visualizations to describe potential customer segments and suggest business implications."
      ],
      "metadata": {
        "id": "dh9VZoFYl_yu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's begin by setting up our environment and loading our customer data!\n"
      ],
      "metadata": {
        "id": "NOxJ0vnSmi4g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Data Acquisition and Initial Exploration\n",
        "\n",
        "For this tutorial, we will first work with a synthetic dataset named ecommerce_customer_data.csv. This file contains anonymized data representing various aspects of customer engagement and purchasing behavior over a period.\n",
        "\n",
        "First, let's ensure we have our necessary libraries installed and then load the dataset."
      ],
      "metadata": {
        "id": "ij3C8orkmkAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from tqdm.autonotebook import tqdm"
      ],
      "metadata": {
        "id": "b7n-ZcjumxEQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    data = pd.read_csv(\"ecommerce_customer_data.csv\")\n",
        "    print(\"Dataset loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Creating synthetic dataset...\")\n",
        "    np.random.seed(42)\n",
        "    num_customers = 500\n",
        "\n",
        "    data = pd.DataFrame({\n",
        "        'CustomerID': np.arange(1, num_customers + 1),\n",
        "        'Age': np.random.randint(18, 70, num_customers),\n",
        "        'Gender': np.random.choice(['Male', 'Female'], num_customers),\n",
        "        'Average_Order_Value': np.random.normal(50, 20, num_customers).round(2).clip(min=5),\n",
        "        'Number_of_Purchases': np.random.randint(1, 30, num_customers),\n",
        "        'Days_Since_Last_Purchase': np.random.randint(1, 180, num_customers),\n",
        "        'Product_Category_Preference': np.random.choice(['Electronics', 'Apparel', 'Books', 'Home Goods', 'Beauty'], num_customers),\n",
        "        'Customer_Lifetime_Value': np.random.normal(200, 100, num_customers).round(2).clip(min=10)\n",
        "    })\n",
        "\n",
        "    data.to_csv(\"ecommerce_customer_data.csv\", index=False)"
      ],
      "metadata": {
        "id": "jKHAWWUYm0Ks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nDataset Head:\")\n",
        "print(data.head())\n",
        "print(\"\\nDataset Info:\")\n",
        "data.info()\n",
        "print(\"\\nDataset Description:\")\n",
        "print(data.describe())"
      ],
      "metadata": {
        "id": "nYs8NzoCm3zA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Feature Engineering and Preprocessing\n",
        "\n",
        "Before we can apply dimensionality reduction techniques, we need to convert all our features into a numerical format and scale them appropriately. This is crucial because algorithms like PCA and t-SNE are sensitive to the magnitude of the features.\n",
        "\n",
        "Here's our plan:\n",
        "* **Drop CustomerID:** It's an identifier and doesn't contain behavioral information.\n",
        "* **One-Hot Encode Categorical Features:** Convert Gender and Product_Category_Preference into numerical representations.\n",
        "* **Standardize Numerical Features:** Scale all numerical features to have a mean of 0 and a standard deviation of 1."
      ],
      "metadata": {
        "id": "MxLBJShFnKjq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features_df = data.drop('CustomerID', axis=1)\n",
        "features_df = pd.get_dummies(features_df, columns=['Gender', 'Product_Category_Preference'], drop_first=True)\n",
        "\n",
        "numerical_cols = ['Age', 'Average_Order_Value', 'Number_of_Purchases',\n",
        "                  'Days_Since_Last_Purchase', 'Customer_Lifetime_Value']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "features_df[numerical_cols] = scaler.fit_transform(features_df[numerical_cols])\n",
        "\n",
        "customer_labels = data['Product_Category_Preference']"
      ],
      "metadata": {
        "id": "Mmv869QKnY5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our data features_df is ready for dimensionality reduction!"
      ],
      "metadata": {
        "id": "3coU5o6hnfBO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Dimensionality Reduction: Principal Component Analysis (PCA)\n",
        "\n",
        "PCA is a linear dimensionality reduction technique that transforms the data into a new coordinate system where the greatest variance by any projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on. It helps us capture the most important information (variance) in fewer dimensions.\n",
        "\n",
        "First, let's look at how much variance each principal component explains."
      ],
      "metadata": {
        "id": "IOS6demanfoe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA()\n",
        "pca.fit(features_df)\n",
        "pca_data = pca.transform(features_df)\n",
        "per_var = np.round(pca.explained_variance_ratio_ * 100, decimals=1)\n",
        "labels_all = ['PC' + str(x) for x in range(1, len(per_var) + 1)]\n",
        "per_var_display = per_var[:10]\n",
        "labels_display = labels_all[:10]\n",
        "with plt.style.context('dark_background'):\n",
        "    plt.figure(figsize=(15, 7))\n",
        "    plt.xlabel(\"Number of Principal Components\")\n",
        "    plt.ylabel(\"Percentage of variance explained\")\n",
        "    plt.bar(range(1, len(per_var_display) + 1), per_var_display, tick_label=labels_display, color=\"cyan\", alpha=0.7)\n",
        "    plt.plot(range(1, len(per_var_display) + 1), np.cumsum(per_var_display), color=\"red\", marker='o', linestyle='--')\n",
        "    plt.scatter(range(1, len(per_var_display) + 1), np.cumsum(per_var_display), color=\"yellow\", s=50)\n",
        "    plt.title(\"Explained Variance by Principal Components\")\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "jFBDpmf4nmlQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Observation:** The first few principal components capture a significant portion of the variance in our customer dataset. The cumulative variance curve shows how many components are needed to explain a certain amount of the total variation.\n",
        "\n",
        "Now, let's visualize our customers using the first two principal components. We'll color the points by their Product_Category_Preference (which we saved earlier) to see if PCA naturally separates customers based on this known characteristic."
      ],
      "metadata": {
        "id": "_bM6ZYvBnvKY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca_df = pd.DataFrame(data=pca_data[:, 0:2], columns=['PC1', 'PC2'])\n",
        "pca_df['Product_Category_Preference'] = customer_labels.values\n",
        "fig = px.scatter(pca_df, x='PC1', y='PC2', color='Product_Category_Preference',\n",
        "                 title=\"Customer Segmentation via PCA (Colored by Product Preference)\",\n",
        "                 labels={'PC1': 'Principal Component 1', 'PC2': 'Principal Component 2'},\n",
        "                 hover_data=['Product_Category_Preference'], # Add hover info\n",
        "                 height=600, width=900,\n",
        "                 color_discrete_sequence=px.colors.qualitative.Bold) # Use a nice color sequence\n",
        "fig.update_layout(\n",
        "    plot_bgcolor='rgba(0,0,0,0)', # Transparent background\n",
        "    paper_bgcolor='rgba(0,0,0,0)',\n",
        "    font_color='white'\n",
        ")\n",
        "fig.show(renderer=\"colab\")"
      ],
      "metadata": {
        "id": "M8d0Z_BMnxma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca_df = pd.DataFrame(pca_data[:, :2], columns=['PC1','PC2'])\n",
        "pca_df['Category'] = customer_labels\n",
        "fig = px.scatter(pca_df, x='PC1', y='PC2', color='Category')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "lEJQFCJ_YIM5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your turn to interpret!\n",
        "\n",
        "**Observations from PCA Plot:**\n",
        "* Do you see any clear separation based on Product_Category_Preference?\n",
        "* Are there any dense clusters, even if they contain mixed preferences?\n",
        "* What does the spread of points suggest about customer behavior?\n",
        "\n",
        "PCA provides a good overall view, but it's a linear method. Sometimes, complex, non-linear relationships between data points are better captured by other techniques."
      ],
      "metadata": {
        "id": "zAbBnoWvn7SI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Dimensionality Reduction: t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
        "\n",
        "t-SNE is a non-linear dimensionality reduction algorithm particularly well-suited for visualizing high-dimensional datasets. It aims to place data points in a low-dimensional space such that points that are close together in the high-dimensional space remain close together in the low-dimensional map, and points that are far apart remain far apart. t-SNE is excellent at revealing local structures and clusters.\n",
        "\n",
        "A key parameter in t-SNE is perplexity. Perplexity relates to the number of nearest neighbors that are considered. It can be thought of as a continuous measure of the number of effective nearest neighbors. A good perplexity value often lies between 5 and 50. Different perplexity values can reveal different aspects of the data structure. n_iter defines the number of iterations for the optimization.\n",
        "\n",
        "Let's apply t-SNE to our features_df and visualize the results."
      ],
      "metadata": {
        "id": "_rmaVsoKoF7y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_state = 42\n",
        "n_components = 2\n",
        "perplexity = 30\n",
        "n_iter = 1000\n",
        "print(f\"Applying t-SNE with perplexity={perplexity}, n_iter={n_iter}...\")\n",
        "model_tsne = TSNE(n_components=n_components, random_state=random_state,\n",
        "                  perplexity=perplexity, n_iter=n_iter, n_jobs=-1, verbose=1)\n",
        "tsne_data = model_tsne.fit_transform(features_df) # Fix: Add this line to compute tsne_data\n",
        "print(\"t-SNE completed.\")\n",
        "tsne_df = pd.DataFrame(data=tsne_data, columns=['TSNE1', 'TSNE2'])\n",
        "tsne_df['Product_Category_Preference'] = customer_labels.values\n",
        "fig = px.scatter(tsne_df, x='TSNE1', y='TSNE2', color='Product_Category_Preference',\n",
        "                 title=f\"Customer Segmentation via t-SNE (Perplexity={perplexity})\",\n",
        "                 labels={'TSNE1': 't-SNE Component 1', 'TSNE2': 't-SNE Component 2'},\n",
        "                 hover_data=['Product_Category_Preference'],\n",
        "                 height=600, width=900,\n",
        "                 color_discrete_sequence=px.colors.qualitative.Bold)\n",
        "fig.update_layout(\n",
        "    plot_bgcolor='rgba(0,0,0,0)',\n",
        "    paper_bgcolor='rgba(0,0,0,0)',\n",
        "    font_color='white'\n",
        ")\n",
        "fig.show(renderer=\"colab\")"
      ],
      "metadata": {
        "id": "dlzHGfueoLsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_tsne = TSNE(n_components=2, perplexity=30, n_iter=1000, random_state=42)\n",
        "tsne_data = model_tsne.fit_transform(features_df)\n",
        "tsne_df = pd.DataFrame(tsne_data, columns=['TSNE1','TSNE2'])\n",
        "tsne_df['Category'] = customer_labels\n",
        "fig = px.scatter(tsne_df, x='TSNE1', y='TSNE2', color='Category')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "rXv4Fa5jYSq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations from t-SNE Plot:**\n",
        "* How does this plot compare to the PCA plot? Is the separation of clusters more distinct?\n",
        "* Can you identify specific customer segments based on the clustering and Product_Category_Preference?\n",
        "* Are there any \"outlier\" points or smaller, distinct clusters that might represent niche customer behaviors?\n",
        "\n",
        "**Experimentation Challenge:**\n",
        "Try changing the perplexity parameter (e.g., to 5, 15, 50, or 100) and re-run the t-SNE code. How does this affect the clusters and overall structure of the plot? Which perplexity value seems to reveal the most interpretable customer segments?"
      ],
      "metadata": {
        "id": "aA78BfBMoUne"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**(Optional) 5. Dimensionality Reduction: Uniform Manifold Approximation and Projection (UMAP)**\n",
        "\n",
        "UMAP is another powerful non-linear dimensionality reduction technique, often faster than t-SNE and sometimes better at preserving both local and global data structure. It's becoming increasingly popular for visualizing complex datasets.\n",
        "\n",
        "To use UMAP, you might need to install it first: !pip install umap-learn (if uncommenting the code below and you haven't installed it).\n"
      ],
      "metadata": {
        "id": "XpvmgoCtoakA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q umap-learn\n",
        "import umap\n",
        "random_state = 42\n",
        "n_components = 2\n",
        "n_neighbors = 15\n",
        "min_dist = 0.1\n",
        "print(f\"Applying UMAP with n_neighbors={n_neighbors}, min_dist={min_dist}...\")\n",
        "model_umap = umap.UMAP(n_components=n_components, random_state=random_state,\n",
        "                     n_neighbors=n_neighbors, min_dist=min_dist, verbose=True)\n",
        "umap_data = model_umap.fit_transform(features_df)\n",
        "print(\"UMAP completed.\")\n",
        "umap_df = pd.DataFrame(data=umap_data, columns=['UMAP1', 'UMAP2'])\n",
        "umap_df['Product_Category_Preference'] = customer_labels.values\n",
        "fig = px.scatter(umap_df, x='UMAP1', y='UMAP2', color='Product_Category_Preference',\n",
        "                 title=f\"Customer Segmentation via UMAP (n_neighbors={n_neighbors}, min_dist={min_dist})\",\n",
        "                 labels={'UMAP1': 'UMAP Component 1', 'UMAP2': 'UMAP Component 2'},\n",
        "                 hover_data=['Product_Category_Preference'],\n",
        "                  height=600, width=900,\n",
        "                  color_discrete_sequence=px.colors.qualitative.Bold)\n",
        "fig.update_layout(\n",
        "    plot_bgcolor='rgba(0,0,0,0)',\n",
        "    paper_bgcolor='rgba(0,0,0,0)',\n",
        "    font_color='white'\n",
        ")\n",
        "fig.show(renderer=\"colab\")"
      ],
      "metadata": {
        "id": "S9RMb6ddoqAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**UMAP Observations:**\n",
        "* If you ran the UMAP code, how do its clusters compare to t-SNE and PCA?\n",
        "* Does it provide an even clearer separation or a different perspective on the customer segments?\n",
        "* Experiment with n_neighbors and min_dist parameters to see how they influence the plot."
      ],
      "metadata": {
        "id": "aMq607S6owlA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Conclusion and Business Implications\n",
        "\n",
        "Congratulations! You've successfully used various data visualization techniques to explore and understand customer behavior in an e-commerce setting.\n",
        "\n",
        "Based on your observations from the PCA, t-SNE, and potentially UMAP plots, you should be able to identify several distinct customer segments. For example:\n",
        "* **High-Value Shoppers:** Customers with high Customer_Lifetime_Value and Average_Order_Value, potentially making frequent purchases. They might cluster together.\n",
        "* **Budget-Conscious Buyers:** Customers with lower Average_Order_Value but possibly high Number_of_Purchases.\n",
        "* **New Customers/Low Engagement:** Customers with high Days_Since_Last_Purchase or low Number_of_Purchases.\n",
        "* **Category Loyalists:** Customers strongly preferring one product category, forming distinct groups."
      ],
      "metadata": {
        "id": "3wQnVbgIo6uK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How would a business use these insights?**\n",
        "\n",
        "Imagine presenting these plots to a marketing team. They could then:\n",
        "* **Target High-Value Shoppers:** Offer exclusive early access to new products or personalized loyalty rewards.\n",
        "* **Re-engage Low Engagement Customers:** Send targeted promotions or surveys to understand their needs and bring them back.\n",
        "* **Cross-Sell to Category Loyalists:** Recommend complementary products from other categories based on their established preferences.\n",
        "* **Identify Product Gaps:** If a category preference is poorly represented, it might indicate a market opportunity or a need to improve offerings.\n",
        "\n",
        "This project highlights the immense value of visualizing high-dimensional data. Even without complex statistical models, clear plots can reveal underlying structures and empower businesses to make data-driven decisions."
      ],
      "metadata": {
        "id": "mbb-vST-pMKw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll continue, building directly on the previous sections by trying it on a real dataset instead of synthetic dataset."
      ],
      "metadata": {
        "id": "g0RHy3A8pfIg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Continuation: Applying Customer Segmentation to Real-World E-commerce Data\n",
        "**Introduction: From Synthetic to Real-World Challenges**\n",
        "\n",
        "You've successfully navigated customer segmentation with a synthetic dataset, mastering the concepts of feature engineering, standardization, PCA, and t-SNE. Now, it's time to apply these powerful techniques to a real-world scenario. Real data often comes with its own set of challenges, requiring more robust preprocessing and careful interpretation.\n",
        "\n",
        "In this section, we will analyze the **\"Online Retail Dataset\"** - a well-known public dataset containing actual transactional data. This will allow us to:\n",
        "* Experience data loading and cleaning for a more complex, real-world dataset.\n",
        "* Derive meaningful features from raw transaction records.\n",
        "* Re-apply dimensionality reduction and visualization to uncover genuine customer segments.\n",
        "* Discuss the business implications based on real purchasing patterns.\n",
        "\n",
        "Let's dive into the complexities and insights offered by real e-commerce data!"
      ],
      "metadata": {
        "id": "4iWAo7l9pqBW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Real Data Acquisition and Initial Exploration: Online Retail Dataset\n",
        "\n",
        "We will download the \"Online Retail Dataset\" from the UCI Machine Learning Repository. This dataset contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based online retail company."
      ],
      "metadata": {
        "id": "cV1y9DB6p8mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from tqdm.autonotebook import tqdm\n",
        "try:\n",
        "    # URL to the dataset (Excel file)\n",
        "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx\"\n",
        "    df_raw = pd.read_excel(url)\n",
        "    print(\"Online Retail dataset downloaded and loaded successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Error downloading or loading dataset: {e}\")\n",
        "    print(\"Please ensure you have 'openpyxl' installed: pip install openpyxl\")\n",
        "    # Fallback to a local file if download fails (e.g., if you've manually downloaded it)\n",
        "    try:\n",
        "        df_raw = pd.read_excel(\"Online Retail.xlsx\")\n",
        "        print(\"Loaded from local 'Online Retail.xlsx' file.\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"Local file 'Online Retail.xlsx' not found either. Please download it manually from:\")\n",
        "        print(\"https://archive.ics.uci.edu/ml/datasets/Online+Retail\")\n",
        "        print(\"And place it in the same directory as this notebook.\")\n",
        "        df_raw = pd.DataFrame() # Create an empty DataFrame to avoid errors later\n",
        "if not df_raw.empty:\n",
        "    print(\"\\nDataset Head:\")\n",
        "    print(df_raw.head())\n",
        "    print(\"\\nDataset Info:\")\n",
        "    df_raw.info()\n",
        "    print(\"\\nDataset Description:\")\n",
        "    print(df_raw.describe())\n",
        "else:\n",
        "    print(\"\\nCannot proceed without dataset. Please resolve the loading issue.\")"
      ],
      "metadata": {
        "id": "DAm9E5zSqBeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset is much larger and more complex! Key observations:\n",
        "\n",
        "* CustomerID **has missing values**: We can't segment customers without an ID, so we'll need to drop these rows.\n",
        "* Quantity **can be negative**: This usually indicates returns or cancellations. We should filter these out for purchase-based segmentation.\n",
        "* UnitPrice **can be negative/zero**: Also likely errors or special cases; we'll remove these.\n",
        "* InvoiceDate is a datetime object, which is good for time-based features.\n",
        "Country is a categorical feature we might use for coloring."
      ],
      "metadata": {
        "id": "x85YZxCHqgTG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Real Data Preprocessing and Feature Engineering (RFM Metrics)\n",
        "\n",
        "For this real-world dataset, we'll engineer classic **RFM (Recency, Frequency, Monetary)** metrics. These are powerful features for customer segmentation:\n",
        "* **Recency (R):** How recently did the customer make a purchase? (Days since last purchase)\n",
        "* **Frequency (F):** How often do they purchase? (Total number of unique invoices)\n",
        "* **Monetary (M):** How much money do they spend? (Total spend)"
      ],
      "metadata": {
        "id": "pBWaH1ffrfe6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we do preprocessing and feature engineering\n",
        "\n",
        "1. **Clean Data:**\n",
        "   * Remove rows with missing CustomerID.\n",
        "   * Remove rows where Quantity is less than or equal to 0 (returns/cancellations).\n",
        "   * Remove rows where UnitPrice is less than or equal to 0.\n",
        "2. **Calculate Total Price:** Quantity * UnitPrice.\n",
        "3. **Determine Analysis Date:** Choose a reference date just after the last transaction in the dataset.\n",
        "4. **Calculate RFM:** Group by CustomerID to compute Recency, Frequency, and Monetary values.\n",
        "5. **Standardize Features:** Apply StandardScaler to RFM values."
      ],
      "metadata": {
        "id": "kGjN8jL9rsL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df_raw.copy()\n",
        "df.dropna(subset=['CustomerID'], inplace=True)\n",
        "df['CustomerID'] = df['CustomerID'].astype(int)\n",
        "df = df[df['Quantity'] > 0]\n",
        "df = df[df['UnitPrice'] > 0]\n",
        "print(f\"Cleaned data shape: {df.shape}\")\n",
        "print(f\"Number of unique customers: {df['CustomerID'].nunique()}\")\n",
        "df['TotalPrice'] = df['Quantity'] * df['UnitPrice']\n",
        "max_invoice_date = df['InvoiceDate'].max()\n",
        "analysis_date = max_invoice_date + pd.Timedelta(days=1)\n",
        "print(f\"Analysis Reference Date: {analysis_date}\")\n",
        "rfm_df = df.groupby('CustomerID').agg(\n",
        "    Recency=('InvoiceDate', lambda date: (analysis_date - date.max()).days),\n",
        "    Frequency=('InvoiceNo', 'nunique'), # Count unique invoices for frequency\n",
        "    Monetary=('TotalPrice', 'sum')\n",
        ").reset_index()\n",
        "print(\"\\nRFM Features Head:\")\n",
        "print(rfm_df.head())\n",
        "print(\"\\nRFM Features Description:\")\n",
        "print(rfm_df.describe())\n",
        "customer_country = df.drop_duplicates(subset=['CustomerID']).set_index('CustomerID')['Country']\n",
        "rfm_df['Country'] = rfm_df['CustomerID'].map(customer_country)\n",
        "rfm_features = rfm_df.drop(['CustomerID', 'Country'], axis=1)\n",
        "scaler = StandardScaler()\n",
        "rfm_scaled = scaler.fit_transform(rfm_features)\n",
        "rfm_scaled_df = pd.DataFrame(rfm_scaled, columns=rfm_features.columns, index=rfm_df.index)\n",
        "print(\"\\nScaled RFM Features Head:\")\n",
        "print(rfm_scaled_df.head())"
      ],
      "metadata": {
        "id": "QuFp9NlTsDUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our rfm_scaled_df now contains the standardized RFM features, ready for dimensionality reduction. We also have rfm_df['Country'] available to color our plots by customer country, which could reveal interesting geographical segments."
      ],
      "metadata": {
        "id": "BaT0pmwCsPGD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Dimensionality Reduction: Principal Component Analysis (PCA) on RFM Data\n",
        "\n",
        "Let's re-apply PCA to our RFM features. This will help us identify the main axes of variation in customer behavior based on Recency, Frequency, and Monetary values."
      ],
      "metadata": {
        "id": "0FsVGmUssRkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca_rfm = PCA()\n",
        "pca_rfm.fit(rfm_scaled_df)\n",
        "pca_rfm_data = pca_rfm.transform(rfm_scaled_df)\n",
        "per_var_rfm = np.round(pca_rfm.explained_variance_ratio_ * 100, decimals=1)\n",
        "labels_all_rfm = ['PC' + str(x) for x in range(1, len(per_var_rfm) + 1)]\n",
        "per_var_rfm_display = per_var_rfm[:3]\n",
        "labels_rfm_display = labels_all_rfm[:3]\n",
        "with plt.style.context('dark_background'):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.xlabel(\"Number of Principal Components\")\n",
        "    plt.ylabel(\"Percentage of variance explained\")\n",
        "    plt.bar(range(1, len(per_var_rfm_display) + 1), per_var_rfm_display,\n",
        "            tick_label=labels_rfm_display, color=\"lightgreen\", alpha=0.7)\n",
        "    plt.plot(range(1, len(per_var_rfm_display) + 1), np.cumsum(per_var_rfm_display),\n",
        "             color=\"red\", marker='o', linestyle='--')\n",
        "    plt.scatter(range(1, len(per_var_rfm_display) + 1), np.cumsum(per_var_rfm_display),\n",
        "                color=\"yellow\", s=50)\n",
        "    plt.title(\"Explained Variance by Principal Components (RFM Data)\")\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.5)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ifyS5Td2sOjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observation:** With only three features (R, F, M), PCA is straightforward. The first PC typically explains a large portion, but all three components are often needed to capture most of the variance."
      ],
      "metadata": {
        "id": "HPOu9KVwsaep"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's visualize the customers in the 2D PCA space, coloring them by Country to see if geographic location plays a role in customer behavior patterns. We'll focus on the top 10 countries by customer count to keep the legend manageable, and group others as 'Other'."
      ],
      "metadata": {
        "id": "DZ623ZZwseE9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca_rfm_df = pd.DataFrame(data=pca_rfm_data[:, 0:2], columns=['PC1', 'PC2'])\n",
        "pca_rfm_df['CustomerID'] = rfm_df['CustomerID'] # Keep CustomerID for merging\n",
        "customer_country_data = rfm_df[['CustomerID', 'Country']]\n",
        "pca_rfm_df = pd.merge(pca_rfm_df, customer_country_data, on='CustomerID', how='left')\n",
        "top_countries = pca_rfm_df['Country'].value_counts().nlargest(10).index\n",
        "pca_rfm_df['Country_Grouped'] = pca_rfm_df['Country'].apply(lambda x: x if x in top_countries else 'Other')\n",
        "fig = px.scatter(pca_rfm_df, x='PC1', y='PC2', color='Country_Grouped',\n",
        "                 title=\"Customer Segmentation via PCA (RFM - Colored by Country)\",\n",
        "                 labels={'PC1': 'Principal Component 1', 'PC2': 'Principal Component 2'},\n",
        "                 hover_data=['CustomerID', 'Country_Grouped', 'PC1', 'PC2'],\n",
        "                 height=700, width=1000,\n",
        "                 color_discrete_sequence=px.colors.qualitative.Alphabet) # Use a good color sequence\n",
        "fig.update_layout(\n",
        "    plot_bgcolor='rgba(0,0,0,0)',\n",
        "    paper_bgcolor='rgba(0,0,0,0)',\n",
        "    font_color='white'\n",
        ")\n",
        "fig.show(renderer=\"colab\")"
      ],
      "metadata": {
        "id": "YFx8F3QksZ9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next natural step is to apply t-SNE and then UMAP to the real RFM data.\n",
        "\n",
        "This will allow us to compare how these non-linear methods perform in revealing customer segments compared to PCA, especially with real-world complexities.\n",
        "\n",
        "Let's continue with t-SNE:"
      ],
      "metadata": {
        "id": "3BygPP8usu4M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Dimensionality Reduction: t-Distributed Stochastic Neighbor Embedding (t-SNE) on RFM Data\n",
        "\n",
        "As we saw with the synthetic data, t-SNE excels at uncovering non-linear relationships and local clusters within the data. With our real RFM features, t-SNE should provide a more nuanced view of customer segments compared to the linear PCA.\n",
        "\n",
        "We'll use the same perplexity and n_iter parameters as a starting point, but remember that experimenting with these values is key to finding the most insightful visualization for your specific dataset."
      ],
      "metadata": {
        "id": "m7s6jmEPsznm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random_state = 42\n",
        "n_components = 2\n",
        "perplexity = 30\n",
        "n_iter = 1000\n",
        "print(f\"Applying t-SNE to RFM data with perplexity={perplexity}, n_iter={n_iter}...\")\n",
        "model_tsne_rfm = TSNE(n_components=n_components, random_state=random_state,\n",
        "                      perplexity=perplexity, n_iter=n_iter, n_jobs=-1, verbose=1)\n",
        "tsne_rfm_data = model_tsne_rfm.fit_transform(rfm_scaled_df)\n",
        "print(\"t-SNE on RFM data completed.\")\n",
        "tsne_rfm_df = pd.DataFrame(data=tsne_rfm_data, columns=['TSNE1', 'TSNE2'])\n",
        "tsne_rfm_df['CustomerID'] = rfm_df['CustomerID']\n",
        "tsne_rfm_df = pd.merge(tsne_rfm_df, customer_country_data, on='CustomerID', how='left')\n",
        "tsne_rfm_df['Country_Grouped'] = tsne_rfm_df['Country'].apply(lambda x: x if x in top_countries else 'Other')\n",
        "fig = px.scatter(tsne_rfm_df, x='TSNE1', y='TSNE2', color='Country_Grouped',\n",
        "                 title=f\"Customer Segmentation via t-SNE (RFM - Perplexity={perplexity})\",\n",
        "                 labels={'TSNE1': 't-SNE Component 1', 'TSNE2': 't-SNE Component 2'},\n",
        "                 hover_data=['CustomerID', 'Country_Grouped', 'TSNE1', 'TSNE2'],\n",
        "                 height=700, width=1000,\n",
        "                 color_discrete_sequence=px.colors.qualitative.Alphabet)\n",
        "fig.update_layout(\n",
        "    plot_bgcolor='rgba(0,0,0,0)',\n",
        "    paper_bgcolor='rgba(0,0,0,0)',\n",
        "    font_color='white'\n",
        ")\n",
        "fig.show(renderer=\"colab\")"
      ],
      "metadata": {
        "id": "hu5AfBYVs5Bi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Observations from t-SNE Plot on RFM Data:**\n",
        "* How do the clusters here compare to the PCA plot? Is the separation generally better defined?\n",
        "* Do certain countries now form more cohesive groups, or are they still mixed?\n",
        "* Can you visually identify distinct customer behavior segments (e.g., a tight cluster of high-frequency buyers vs. a dispersed group of infrequent purchasers)?\n",
        "\n",
        "**Experimentation Challenge (Important!):**\n",
        "\n",
        "Just like with the synthetic data, the perplexity value is crucial for t-SNE. Re-run the t-SNE code cell with different perplexity values (e.g., 5, 15, 50, 100, 200). Observe how the clustering changes. Which perplexity value do you think gives the most meaningful and stable representation of customer segments in this real dataset? Why?"
      ],
      "metadata": {
        "id": "3qZNys2OtBHW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Dimensionality Reduction: Uniform Manifold Approximation and Projection (UMAP) on RFM Data\n",
        "\n",
        "Let's now apply UMAP, which often offers a good balance between preserving local and global structure and is generally faster than t-SNE. We'll continue to color by Country_Grouped."
      ],
      "metadata": {
        "id": "a3Q-TJMdtPCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import umap\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "random_state = 42\n",
        "n_components = 2\n",
        "n_neighbors = 15\n",
        "min_dist = 0.1\n",
        "print(f\"Applying UMAP to RFM data with n_neighbors={n_neighbors}, min_dist={min_dist}...\")\n",
        "model_umap_rfm = umap.UMAP(\n",
        "    n_components=n_components,\n",
        "    random_state=random_state,\n",
        "    n_neighbors=n_neighbors,\n",
        "    min_dist=min_dist,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "umap_rfm_data = model_umap_rfm.fit_transform(rfm_scaled_df)\n",
        "print(\"UMAP on RFM data completed.\")\n",
        "umap_rfm_df = pd.DataFrame(umap_rfm_data, columns=['UMAP1', 'UMAP2'])\n",
        "umap_rfm_df['CustomerID'] = rfm_df['CustomerID']\n",
        "umap_rfm_df = umap_rfm_df.merge(customer_country_data, on='CustomerID', how='left')\n",
        "umap_rfm_df['Country_Grouped'] = umap_rfm_df['Country'].apply(\n",
        "    lambda x: x if x in top_countries else 'Other'\n",
        ")\n",
        "fig = px.scatter(\n",
        "    umap_rfm_df,\n",
        "    x='UMAP1',\n",
        "    y='UMAP2',\n",
        "    color='Country_Grouped',\n",
        "    title=f\"Customer Segmentation via UMAP (RFM â€” n_neighbors={n_neighbors}, min_dist={min_dist})\",\n",
        "    labels={'UMAP1': 'UMAP Component 1', 'UMAP2': 'UMAP Component 2'},\n",
        "    hover_data=['CustomerID', 'Country_Grouped', 'UMAP1', 'UMAP2'],\n",
        "    height=700,\n",
        "    width=1000,\n",
        "    color_discrete_sequence=px.colors.qualitative.Alphabet\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    plot_bgcolor='rgba(0,0,0,0)',\n",
        "    paper_bgcolor='rgba(0,0,0,0)',\n",
        "    font_color='white'\n",
        ")\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "3t-j_YHaj9Ub"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**UMAP Observations on RFM Data:**\n",
        "* In case you have uncommented and run UMAP, how does UMAP's representation of the clusters compare to both PCA and t-SNE? Does it show a clearer global structure or sharper local clusters?\n",
        "* Are there any \"bridges\" or connections between clusters that UMAP highlights better than t-SNE?\n",
        "* Consider how different n_neighbors (e.g., 5, 50, 100) and min_dist (e.g., 0.0, 0.5) values might alter the UMAP embedding."
      ],
      "metadata": {
        "id": "P0NUaiC1titb"
      }
    }
  ]
}